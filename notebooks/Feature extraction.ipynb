{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0027831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "import torchy\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd56f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f64922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added :cartoon, length: 2344, classes: 7\n",
      "Added :art_painting, length: 2048, classes: 7\n",
      "Added :photo, length: 1670, classes: 7\n",
      "Added :sketch, length: 3929, classes: 7\n"
     ]
    }
   ],
   "source": [
    "# means and standard deviations ImageNet because the network is pretrained\n",
    "means, stds = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "\n",
    "# Define transforms to apply to each image\n",
    "transf = transforms.Compose([ #transforms.Resize(227),      # Resizes short size of the PIL image to 256\n",
    "                              transforms.CenterCrop(224),  # Crops a central square patch of the image 224 because torchvision's AlexNet needs a 224x224 input!\n",
    "                              transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
    "                              transforms.Normalize(means,stds) # Normalizes tensor with mean and standard deviation\n",
    "])\n",
    "\n",
    "data_root = 'Data/PACS/'\n",
    "\n",
    "\n",
    "datasets = {}\n",
    "for name in os.listdir(data_root):\n",
    "    \n",
    "    if not name[0] == '.':\n",
    "        datasets[name] = torchvision.datasets.ImageFolder(data_root+name, transform=transf)\n",
    "        print(f\"Added :{name}, length: {len(datasets[name])}, classes: {len(datasets[name].classes)}\")\n",
    "    \n",
    "# Check dataset sizes\n",
    "# print(f\"Clipart Dataset: {len(clipart_dataset)}, classes: {len(clipart_dataset.classes)}\")\n",
    "# print(f\"Quickdraw Dataset: {len(qd_dataset)}, classes: {len(qd_dataset.classes)}\")\n",
    "# # print(f\"Cartoon Dataset: {len(cartoon_dataset)}\")\n",
    "# print(f\"Sketch Dataset: {len(sketch_dataset)}, classes: {len(sketch_dataset.classes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7108f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "dataloaders = {}\n",
    "for name in datasets.keys():\n",
    "    dataloaders[name] = DataLoader(datasets[name], batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f3d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import resnet18\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "modules=list(resnet18.children())[:-1]\n",
    "resnet18=nn.Sequential(*modules)\n",
    "for p in resnet18.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eb7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165778ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c7249",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasets['cartoon'].classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f2d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, dataloaders, datasets):\n",
    "    \n",
    "    feature_dict = {}\n",
    "    for domain in datasets.keys():\n",
    "        print(domain)\n",
    "        feature_dict[domain] = {}\n",
    "\n",
    "        for i in range(len(datasets[domain].classes)):\n",
    "            feature_dict[domain][i] = []\n",
    "\n",
    "        for j, batch in enumerate(dataloaders[domain]):\n",
    "            print(f\"Batch: {j}\")\n",
    "            input, classes = batch\n",
    "            \n",
    "            features = model(input)\n",
    "            for i, c in enumerate(classes):\n",
    "                feature_dict[domain][int(c)].append(features[i])\n",
    "\n",
    "    return feature_dict\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "feature_dict = extract_features(resnet18, dataloaders, datasets)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"PACS.pkl\", \"wb\")\n",
    "pickle.dump(feature_dict, a_file)\n",
    "a_file.close()\n",
    "\n",
    "tmp = pd.DataFrame.from_dict(feature_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be521791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe651045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d393f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(feature_dict['photo'][2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca71b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
